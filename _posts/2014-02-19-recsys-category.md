---
layout: post
title: 推荐算法分类
categories:
- Machine Learning
tags:
- 推荐算法
- 分类
- 总结

---

## Item based collective filtering

总结：物以类聚

应用场景：item相对固定，且item数相对user更少。如商城网站、影视类推荐、图书类推荐、音乐推荐等。

方法：离线计算item的相似度矩阵供线上使用

缺点：由于基于item的相似性，故推荐的item相似，缺乏多样性
 
## user based collective filtering

总结：人以群分,找和用户有相同品味的其他用户

适用范围：item更新频繁的应用，如: 资讯类服务的应用(以指阅，无觅为代表)等。

方法：通过相似用户喜欢的item推荐给该用户

缺点：相似用户群比较敏感，要频繁地计算出用户的相似用户矩阵，运算量会非常大。推荐的大多是大家都喜欢的热门推荐，有点趋于大众化了
  
## content based

方法：通过文本挖掘方法，提取Item内容(如:媒体的简介，资讯的内容，宝贝的介绍等)的关键词计算相似度; 通过用户的浏览或搜索等行为，提取用户的偏好，也可以在用户注册时显式获取，如让他选择感兴趣的主题或是对看过的影片打分等(当然这些影片都是随机选取的，以保证多样性)。

好处：可以解决Item的冷启动问题; 不需要user-item的评分矩阵，所以没有数据稀疏问题
   
## slope one

方法：Slope One的基本概念很简单，例子1，用户X，Y和A都对Item1打了分。 同时用户X,Y还对Item2打了分，用户A对Item2可能会打多少分呢?

User |Rating to Item 1|    Rating to Item 2
   X |   5 |  3
   Y |   4 |  3
   A |   4 |  ?

根据SlopeOne算法，应该是:4 - ((5-3) + (4-3))/2 = 2。5。

优点：简单，快速

缺点：效果一般，因为算法总是把你的口味与大众的平均口味作对等，容易让推荐结果趋向一致，也即是大从的平均口味
    
## svd

Singular Value Decomposition（奇异值分解）

总结：擒贼先擒王。 抓主要矛盾，忽略次要矛盾，与PCA(主成分分析)类似

方法：这个方法是提取一般实矩阵“特征值”的算法,（这里特征值加引号是因为，特征值是针对方阵来定义的，而一般的$mxn$的实矩阵是没有特征值的。）将一个$mxn$的实矩阵和它的转置相乘，就会得到一个方阵，然后对这个方阵做特征值分解，得到的特征值就是所谓的奇异值的平方。拿到奇异值后，我们就可以抓到主要的成分，丢掉次要和非常次要的成分进行分析。也就是说，我们可以对原来的庞大的常常又非常稀疏的矩阵进行降维和分解，而分解后得到的矩阵都是稠密矩阵。最终我们会得到一个表示user特性的矩阵和一个表示item特性的矩阵。拿到这些数据之后，我们就可以进行推荐了，而且也可以很容易地进行聚类分析。

好处:可以解决rating矩阵的稀疏性问题，同时可以降低矩阵的维度，提高运算速度。

缺点:是付出的空间代价太大

     
## 聚类算法

总结：降低维度以及为并行计算作准备

方法：拿到rating矩阵之后，可以通过这些评分将用户自然地聚成几簇，然后用上述的算法对各个簇做推荐算法并行计算，充分地利用好所有计算资源。当然你也可以在svd分解之后，拿到user和item矩阵之后，对这两个矩阵分别作聚类分析，你可以得到user的簇以及item的簇。这样的结果会非常有意义，你可以作好友推荐，相似item推荐等等。

在基于内容的算法中，因为很多资讯之间并不是那么的相关，把他们都相互计算相似度，会得到很多的0，所以没有必要。因此可以在计算之前，对整个item做个聚类，然后分别对各簇来做相似度计算。
最简单的就是k-means。
      
## 组合算法

总结：博采众长

方法：任何一个算法都有它独特的优势和固有的缺陷，因此单用一个算法的web应用很少，往往是将各种算法组合起来用。

*   将多种算法计算出来的结果，加权之后排序推荐给用户。
*   将多种算法计算出来的结果，各取前几个推荐给用户，这样做的好处是结果很丰富多彩。
*   用svd算法填充后的矩阵作为输入，用普通cf做计算来输出，然后排序推荐。这种叫做层次推荐，可以得到两种方法的好处。
*   对新用户做基于内容的推荐，因为新用户没有任何评分数据，对老用户用cf来做。

## 参考资料

 [1]. [推荐引擎的几种算法总结（转）](http://eric-gcm.iteye.com/blog/1940273)

 [2]. [推荐算法分类](http://blog.csdn.net/wolfguypan/article/details/12587731)

-全文完-

